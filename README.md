# PPEPFinder
PPEPFinder (Plant Pathogen Effector Protein Finder), an integrated deep learning framework designed to predict effector proteins of fungi and oomycetes. PEPFinder consists of three individual predictive models: (i) a sequence-based Transformer model that utilizes rich semantic embeddings generated by a pre-trained PLM (i.e., ESM) as input; (ii) two structure-based Graph Attention Network (GAT) models that represent protein structures as residue contact graphs using the ESM-generated embeddings or the structure pre-trained model SaProt embeddings as node features. To integrate the predictive capabilities of the three individual models, PPEPFinder further employs a logistic regression model to aggregate their outputs into a final prediction score.
# Requirements
- `GAT.yml`: Environment configuration file for the Graph Attention Network (GAT)-based model.
- `deep_transformer.yml`: Environment configuration file for the transformer-based model.
- `SaProt Environment` The environment configuration follows the setup used in the SaProt repository (https://github.com/westlake-repl/SaProt).   
# Dataset
- We provided effector protein datasets for two species, including fungi and oomycetes. data/ contains all the datasets.
The positive and negative samples used for training, respectively. The positive-to-negative sample ratio is approximately 1:3. The dataset is randomly divided into 80% for the training set and 20% for the independent test set. A 5-fold cross-validation is performed on the training set to evaluate model performance. 

- The additional_dataset.fasta contains newly reported effector proteins were manually collected from publications published between January 1, 2023 and December 31, 2024.

- 'Fungal_addtional_dataset' (54 positive and 162 negative samples) and 'Oomycete_additional_dataset' (72 positive and 216 negative samples)
# Seq_Based_Model
The Seq_Based_Model is mainly based on the DeepSecE model (Zhang et al., 2023) (https://github.com/zhangyumeng1sjtu/DeepSecE). Specifically, the model employs ESM as a pretrained protein language model to extract initial embedding representations from input protein sequences. These embeddings are then passed through a Transformer layer to further capture and learn sequence-level features for downstream prediction tasks.
# GAT_Based_Model
The structure-based GAT model extracts spatial information from predicted protein structures and integrates it with node embeddings generated by pre-trained models to construct a graph.
- `fungi/GAT_Based_Model/example_data`contains example data for GAT model demonstration.
# Logistic Regression Model
To further improve predictive performance, an LR model is employed to perform weighted integration of prediction scores generated by multiple independent models, thereby producing a final composite prediction score. 
- `S1(ESM_Transformer)`, `S2(ESM_GAT)`, and `S3(SaProt_GAT)`
